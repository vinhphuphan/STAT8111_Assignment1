---
title: "STAT8111 Assigment1"
author: "Phan Vinh Phu 45747989"
date: "2023-08-18"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r global-option, include=FALSE}
knitr::opts_chunk$set(fig.width=10,fig.height=5,fig.align='left',warning=FALSE,message=FALSE,echo=TRUE)
```

```{r  setup, include=FALSE}
library(tidyverse)
library(GGally)
library(ISwR)
library(corrplot)
library(ggplot2)
library(extraDistr)
```

# Question 1
```{r read the data, include=FALSE }
data <- cystfibr
head(data)
```

## a.Examine first graphically and numerically correlation between variables, then comment :

### Numerical correlation
```{r numerical correlation, include=TRUE }
num_cor <- round(cor(data), 4)
num_cor
```
\pagebreak

### Graphical correlation
```{r graphical correlation, include=TRUE }
corrplot(num_cor, type="lower")
```
### Comment :
From the above correlation plot and correlation table, it can be seen that :

- There might be a strong positive linear relationship between `age` and `height`, `weight` and `pemax`. Beside that, `age` is negatively correlated with `frc`.

- `height` and `weight` are strongly correlated. In addition, `weight` has moderate negative correlation with `rv`, `frc`, `tlc` and positive correlation with `bmp`, `pemax`.

- Similarly, `fev1` has moderate negative correlation with `rv`, `frc`, `tlc`.

- Lastly, `rv` highly positively correlated with `frc`.

## b. the relationship between `weight` and `pemax`

In this part, the relationship between `weight` and `pemax` will be examined.Specifically, `pemax` is the dependent variable (Y) and `weight` is the independent variable (X).

\pagebreak

### Scatter Plot
```{r scatter Plot, include=TRUE }
plot(x = data$weight, y = data$pemax, xlab="weight", ylab = "pemax")
```
The graph illustrates that `weight` is positively related with `pemax`. The trend is that when `weight` increase, `pemax` will increase.

### Linear Model
```{r linear Model, include=TRUE }
model <- lm(pemax ~ weight, data = data)
summary(model)
```

#### The model equation

$\hat{pemax} = 63.5456 + 1.1867 weight +  \epsilon$ $\>$$( \epsilon \sim N(0, \sigma^2))$ 

#### Model Fit

- The $R^2 = 0.4035$ which means 40.35% of variation in `pemax` can be explained by `weight`. This show that the model is not fit well.

#### Model interpretation

- According to the equation, for each unit increase in weight, the pemax will increase about 1.1867.   

- `weight` is significant predictor since the p-value = 0.000646 (< 0.001).

#### Diagnostic Checking

```{r diagnostic, include=TRUE }
par(mfrow = c(1,2))
plot(model, which = c(1,2))
```
- The standardized Residuals versus Fitted values plot appears to be a random scatter about zero, so the model is adequate.This graph also show some residuals which are 14, 17, 25 are low and high. This can be evidence of small amount of heteroscedasticity.

- the Normal Q-Q plot is approximately linear, so it can be said that the normality assumption holds.

\pagebreak

## c. Include in the previous model the sex variable
### Model 2
```{r model 2, include=TRUE }
model_2 <- lm(pemax ~ sex, data = data)
summary(model_2)
```

**The model 2 equation** $\hat{pemax} = 117.50 +(-19.05)sex_i  +  \epsilon$ $\>$ $( \epsilon \sim N(0, \sigma^2))$ 

**Model 2 analysis**

- This model only includes the "sex" variable as a predictor for "pemax."

- For p-value = 0.162, `sex` is insignificant predictor since there is no clear relationship between `sex` and `pemax`.

- The $R^2 = 0.08327$ indicates that the model is poor, only 8.32% of variation in `pemax` can be explained by `sex`.

- The F-statistic is not highly significant (p-value: 0.1618), suggesting that the model might not be a good fit for the data.

\pagebreak

### Model 3
**Check the interaction plot**
```{r interaction plot, include=TRUE}
# Create an interaction plot
ggplot(data, aes(x = weight, y = pemax, color = factor(sex))) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  labs(title = "Interaction: Weight and Pemax by Sex",
       x = "Weight", y = "Pemax", color = "Sex") +
  theme_minimal()
```

The lines of graph are not parallel which shows that there is a significant interaction between `weight` and `pemax` with respect to `sex`. This interaction effect indicates that the relationship between `weight` and `pemax` variable depends on the sex of the individual. This suggests that there might be an interaction effect between `weight` and `sex` in predicting `pemax`.

```{r model 3, include=TRUE }
model_3 <- lm(pemax ~ weight*sex, data = data)
summary(model_3)
```

**The model 3 equation**

$\hat{pemax} = 61.3603 + 1.3572weight + 22.0905sex + (-0.9240) weight\times sex  +  \epsilon$ $\>$ $( \epsilon \sim N(0, \sigma^2))$ 


**Model 3 analysis**

- Model 3 includes both `weight` and `sex` as predictors for `pemax` as well as an interaction term between them.

- Only `weight` is the significant predictor with p-value = 0.0008. Beside that, `sex` and the interaction term are insignificant. This means The interaction is insignificant in this case.

-  The $R^2 = 0.477$ is higher than in Model 2, indicating that this model explains more variability in the response.

- The F-statistic is significant (p-value: 0.003025), suggesting that the overall model is a better fit than a model without predictors.


### Conclusion

Based on the analysis, Model 3 (including both "weight" and "sex" with interaction) seems to be a better fit for predicting "pemax" compared to Model 1 (including `weight`) and Model 2 (only "sex").  Model 2 (with  $R^2 = 0.477$ and $adjusted R^2 = 0.4023$ ) provides more insight into the relationship between the predictors and the response, and it demonstrates a better overall fit to the data.

## d. Construct a statistical model for the response variables pemax based on the normal response distribution and the weight, bmp, fev1, rv,frc.

We are interested in predicting `pemax` (maximal expiratory pressure) by using `weight`, `bmp`, `fev1`, `rv`,`frc`. In this case, the stepwise backward selection will be used to find the best model. In stepwise backward selection, people first regress with all predictor variables in the model. Then, the predictors with the largest p-value in the t-test will be dropped. Next step is fitting the reduced model. This progess will be run iteratively until all variables in the model are significant.Let's start with the full model.

### Full Model
```{r fullmodel, include=TRUE }
full_model <- lm(pemax ~ weight + bmp + fev1 + rv + frc , data = data)
summary(full_model)
```

It can be seen that `frc` has the largest p-value which is 0.937703. Therefore, `frc` explains the least variation when added to the model. Now , `frc` will be dropped.

### The reduced model without `frc`
```{r reduced without frc, include=TRUE }
reduced_model <- lm(pemax ~ weight + bmp + fev1 + rv , data = data)
summary(reduced_model)
```

Similarly, `rv` is the insignificant predictor and has the largest P-value which is 0.146178. Hence, I drop `rv` and fit the reduced model without `frc` and `rv`.


### The reduced model without `frc` and `rv`
```{r reduced without frc and rv, include=TRUE }
reduced_model_2 <- lm(pemax ~ weight + bmp + fev1 , data = data)
summary(reduced_model_2)
```
At this stage, all predictors are significant, therefore, selection process stops here. Nevertheless, there is moderate multi-collinearity since `bmp`, `weight`, `fev1` are positively correlated.

```{r correlation_matrix, include=TRUE}
# Calculate the correlation matrix
correlation_matrix <- cor(data[, c("weight", "bmp", "fev1")])

# Print the correlation matrix
print(correlation_matrix)
```
### Diagnostic Checking

```{r diagnostic 2, include=TRUE }
par(mfrow = c(1,2))
plot(reduced_model_2, which = 1:2)
```   

- The Residuals vs Fitted plot look like random scatter around 0. Therefore, there is no obvious pattern in any of the residual plots so it appears the linearity and constant variance assumptions of the multiple linear model are justified.

- The quantile plot of residuals look approximately linear, suggesting the normality assumption for
residuals is appropriate.

### The final model equation

$\hat{pemax} = 126.3336 + 1.5365weight + (-1.4654)bmp + 1.1086fev1  +  \epsilon$  $\>$ $( \epsilon \sim N(0, \sigma^2))$ 

### Interpretaion
From above equation, 

-  For every unit increase in `weight` (`bmp` and `fev1` stay unchanged), the average `pemax` is expected to increase by 1.5365 units.

- For every unit increase in `bmp` (holding `weight` and `fev1` constant), the average `pemax` is expected to decrease by 1.4654 units.

- For one extra unit of `fev1` , the average `pemax` is expected to increase by 1.1086 units.

- The R-squared value of 0.57 indicates that approximately 57% of the variability in the `pemax` can be explained by the predictor variables (`weight`, `bmp`, and `fev1`) included in the model.

- The final mode has a balance between model complexity and goodness of fit.

# Question 2
## a.Show that the Inverse Gaussian distribution is a member of the exponential family

The Inverse Gaussian distribution is defined a

\begin{align*}
f(x ; \mu, \gamma) &= \sqrt{\frac{\gamma}{2\pi x^3}} \exp\left(-\frac{\gamma(x - \mu)^2}{2\mu^2x}\right)\\ \\
f(x ; \mu, \gamma) &= \exp\left(\frac{1}{2}\log\left(\frac{\gamma}{2\pi x^3}\right)-\frac{\gamma}{2\mu^2}\frac{(x - \mu)^2}{x}\right) \\ \\
f(x ; \mu, \gamma) &= \exp\left(\frac{1}{2}\log\left(\frac{\gamma}{2\pi x^3}\right)-\frac{\gamma}{2\mu^2}\frac{(x^2 - 2x\mu + \mu^2)}{x}\right) \\ \\
f(x ; \mu, \gamma) &= \exp\left(\frac{1}{2}\log\left(\frac{\gamma}{2\pi x^3}\right)-\frac{\gamma x}{2\mu^2}+\frac{x}{\mu} +\frac{\gamma}{2x}\right) \\ \\
f(x ; \mu, \gamma) &= \exp\left(-\frac{\gamma x}{2\mu^2}+\frac{x}{\mu}+\frac{1}{2}\log\left(\frac{\gamma}{2\pi x^3}\right) +\frac{\gamma}{2x}\right)
\end{align*}

The simpler form of the pdf:
\[ f(y ; \theta, \phi) = \exp\left(\frac{y\theta - b(\theta) }{a(\phi)}+c(y, \phi)\right) \]

in this case :
\[\frac{y\theta - b(\theta) }{a(\phi)}+c(y, \phi) = -\frac{\gamma x}{2\mu^2}+\frac{x}{\mu}+\frac{1}{2}log(\frac{\gamma}{2\pi x^3}) +\frac{\gamma}{2x}\]

with: \[\phi = \frac{1}{\gamma} , \> a(\phi) = \frac{1}{\gamma}, \> \theta = -\frac{1}{2\mu^2}, \>b(\theta) = 2\theta^\frac{1}{2} , c = \frac{1}{2}log(\frac{\theta}{2\pi x^3}) +\frac{\theta}{2x}\].

## b. Give the natural parameter and the scale parameter

In the context of the exponential family representation of the Inverse Gaussian distribution, the natural parameter and the scale parameter can be identified as follows :

**Natural Parameter** is $-\frac{\gamma}{2\mu^2}$.

**Scale Parameter** is $\mu$.

## c. Derive the mean and variance of Inverse Gaussian distribution.

**Mean**
\begin{align*}
b'(\theta) &= \frac{1}{\sqrt{2\theta}}, \quad \text{where } \theta = -\frac{1}{2\mu^2}. \\
\text{Substitute } \theta &= -\frac{1}{2\mu^2}: \\
b'(\theta) &= \frac{1}{\sqrt{2 \left( -\frac{1}{2\mu^2} \right)}} \\
\text{Simplify:} \\
b'(\theta) &= \frac{1}{\sqrt{-\frac{1}{\mu^2}}} = \frac{1}{\frac{1}{\mu}} = \mu \\
\text{So, } E[X] &= \mu \\
\end{align*}
**Variance**

\begin{align*}
Var(Y) &= b''(\theta)\phi \\ \\
Var(Y) &= \phi\theta^{-\frac{3}{2}} \\ \\
Recall\>that\>\theta^{-\frac{1}{2}} &= \mu\> and\> \phi = \frac{1}{\gamma}\\ \\
\Rightarrow\>Var(Y) &= \frac{\mu^3}{\gamma}
\end{align*}

# Question 3

The normal linear model is
\[Y_i = x_i^T\beta + \epsilon_i \quad \text{for } i = 1, \ldots, n\]
where \(\beta = (\beta_1, \ldots, \beta_p)^T\) is the \(p\)-dimensional regressor parameter, and the \(\epsilon_i\) are the noise of the model.

## a.

Given the Normal distribution for the noise \(\epsilon_i\) as \(N(0, \sigma^2)\), the likelihood of \(y_i\) is:

\[L_i(\beta) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{1}{2}\left(\frac{y_i - x_i^T\beta}{\sigma}\right)^2\right)\]

The likelihood of the entire sample \(y\) is the product of the individual likelihoods:

\[L(\beta) = \prod_{i=1}^{n} L_i(\beta) = \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{1}{2}\left(\frac{y_i - x_i^T\beta}{\sigma}\right)^2\right)\]

Now, let's consider the log-likelihood:

\begin{align*}
l(\beta) &= \sum_{i=1}^{n} \log\left(\frac{1}{\sigma\sqrt{2\pi}}\right) - \frac{1}{2}\left(\frac{y_i - x_i^T\beta}{\sigma}\right)^2 \\
&= -n \log\left(\sigma\sqrt{2\pi}\right) - \frac{1}{2\sigma^2}\sum_{i=1}^{n} \left(y_i - x_i^T\beta\right)^2
\end{align*}

To find the maximum likelihood estimator of \(\beta\), we want to maximize \(l(\beta)\) with respect to \(\beta\). Since maximizing the log-likelihood is equivalent to maximizing the likelihood itself, we need to find the value of \(\beta\) that maximizes the expression above.

Maximizing \(l(\beta)\) with respect to \(\beta\) is equivalent to minimizing the following expression, which is proportional to the squared error loss:

\[LS(\beta) = \frac{1}{n}\sum_{i=1}^{n} \left(y_i - x_i^T\beta\right)^2\]

Therefore, the maximum likelihood estimator of the parameter \(\beta\) is also the solution of the squared error loss, known as the least square error (LS), given by \(LS(\beta)\). This demonstrates the connection between the maximum likelihood estimation and the least squares estimation.

## b.

Given the Laplace distribution for the noise \(\epsilon_i\) as \(L(0, \sigma^2)\), the likelihood of \(y_i\) is:

\[L_i(\beta) = \frac{1}{2\sigma} \exp\left(-\frac{|y_i - x_i^T\beta|}{\sigma}\right)\]

The likelihood of the entire sample \(y\) is the product of the individual likelihoods:

\[L(\beta) = \prod_{i=1}^{n} L_i(\beta) = \prod_{i=1}^{n} \frac{1}{2\sigma} \exp\left(-\frac{|y_i - x_i^T\beta|}{\sigma}\right)\]

Now, let's consider the log-likelihood:

\begin{align*}
l(\beta) &= \sum_{i=1}^{n} \log\left(\frac{1}{2\sigma}\right) - \frac{|y_i - x_i^T\beta|}{\sigma} \\
&= -n \log(2\sigma) - \frac{1}{\sigma}\sum_{i=1}^{n} |y_i - x_i^T\beta|
\end{align*}

To maximize \(l(\beta)\) with respect to \(\beta\), we want to maximize the negative of the absolute error loss, which is equivalent to minimizing the absolute error loss itself:

\[AL(\beta) = \frac{1}{n}\sum_{i=1}^{n} |y_i - x_i^T\beta|\]

Therefore, in the case of a Laplace distribution error noise, maximizing the log-likelihood is equivalent to minimizing the absolute error loss (AL) defined by \(AL(\beta)\). This demonstrates the connection between the maximum likelihood estimation and the minimization of the absolute error loss for the Laplace distribution case.

## c. 

```{r plot, include=TRUE}
# Parameters
mu <- 0         # Mean for both distributions
sigma <- 1      # Standard deviation for the Normal distribution
scale <- sqrt(2) * sigma  # Scale parameter for Laplace distribution

# Values for x-axis
x <- seq(-10, 10, length.out = 500)

# Calculate pdf values for Normal and Laplace distributions
pdf_normal <- dnorm(x, mean = mu, sd = sigma)
pdf_laplace <- 1 / (2 * scale) * exp(-abs(x - mu) / scale)

# Plotting
plot(x, pdf_normal, type = "l", col = "blue", xlab = "x", ylab = "Probability Density",
     main = "PDF Comparison: Normal vs Laplace Distribution")
lines(x, pdf_laplace, col = "red")
legend("topright", legend = c("Normal Distribution", "Laplace Distribution"),
       col = c("blue", "red"), lty = 1)
```

\pagebreak

## d. 

The claim that the linear model using Laplace error noise is more robust to outliers can be supported by observing the behavior of the probability density functions (pdfs) of the Normal and Laplace distributions in the previous plot.

Here are some arguments based on the plot to support the claim:

- **Heavier Tails of the Laplace Distribution** : In the plot, it can be seen that the Laplace distribution's pdf has heavier tails compared to the Normal distribution. This means that the Laplace distribution assigns more probability to extreme values (outliers) compared to the Normal distribution. 

- **Reduced Influence of Outliers** : In the Laplace distribution, the tail behavior implies that outliers will have a smaller impact on the model estimation compared to the Normal distribution. Outliers that are far from the mean will contribute less to the overall loss, making the model less sensitive to extreme observations. On the other hand, the Normal distribution's lighter tails make it more sensitive to outliers.

- **Minimization of Absolute Errors** : The Laplace distribution's density has a sharp peak at the mean, resulting in a higher concentration of values around the center. When you minimize the absolute error loss, which the Laplace distribution corresponds to, you effectively prioritize minimizing the impact of large individual errors (outliers) rather than the sum of squared errors as in the Normal distribution case. This aligns well with the robustness against individual extreme observations.
